{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find duplicate questions on StackOverflow by their embeddings\n",
    "\n",
    "In this assignment you will learn how to calculate a similarity for pieces of text. Using this approach you will know how to find duplicate questions from [StackOverflow](https://stackoverflow.com)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries\n",
    "\n",
    "In this task you will you will need the following libraries:\n",
    "- [StarSpace](https://github.com/facebookresearch/StarSpace) — a general-purpose model for efficient learning of entity embeddings from Facebook\n",
    "- [Gensim](https://radimrehurek.com/gensim/) — a tool for solving various NLP-related tasks (topic modeling, text representation, ...)\n",
    "- [Numpy](http://www.numpy.org) — a package for scientific computing.\n",
    "- [scikit-learn](http://scikit-learn.org/stable/index.html) — a tool for data mining and data analysis.\n",
    "- [Nltk](http://www.nltk.org) — a platform to work with human language data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "The following cell will download all data required for this assignment into the folder `week3/data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24dbc7efc82445ae9db84827416ae486",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=119127793), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfd54e33e3184023ba074a2a4ea29959",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=535543630), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00157e4f0b8947ee8c53d18602ebd756",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=46408910), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5572cd9f5fce40b2a6669e4451c798d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5333), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from common.download_utils import download_week3_resources\n",
    "\n",
    "download_week3_resources()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grading\n",
    "We will create a grader instace below and use it to collect your answers. Note that these outputs will be stored locally inside grader and will be uploaded to platform only after running submiting function in the last part of this assignment. If you want to make partial submission, you can run that cell any time you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from grader import Grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "grader = Grader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embedding\n",
    "\n",
    "To solve the problem, you will use two different models of embeddings:\n",
    "\n",
    " - [Pre-trained word vectors](https://code.google.com/archive/p/word2vec/) from Google which were trained on a part of Google News dataset (about 100 billion words). The model contains 300-dimensional vectors for 3 million words and phrases. `GoogleNews-vectors-negative300.bin.gz` will be downloaded in `download_week3_resources()`.\n",
    " - Representations using StarSpace on StackOverflow data sample. You will need to train them from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's always easier to start with pre-trained embeddings. Unpack the pre-trained Goggle's vectors and upload them using the function [KeyedVectors.load_word2vec_format](https://radimrehurek.com/gensim/models/keyedvectors.html) from gensim library with the parameter *binary=True*. If the size of the embeddings is larger than the avaliable memory, you could load only a part of the embeddings by defining the parameter *limit* (recommended: 500000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"GoogleNews-vectors-negative300.bin.gz\"\n",
    "path = str(Path().resolve() / \"data\" / filename)\n",
    "wv_embeddings = KeyedVectors.load_word2vec_format(path, binary=True, limit=500000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to work with Google's word2vec embeddings?\n",
    "\n",
    "Once you have loaded the representations, make sure you can access them. First, you can check if the loaded embeddings contain a word:\n",
    "    \n",
    "    'word' in wv_embeddings\n",
    "    \n",
    "Second, to get the corresponding embedding you can use the square brackets:\n",
    "\n",
    "    wv_embeddings['word']\n",
    " \n",
    "### Checking that the embeddings are correct \n",
    " \n",
    "To prevent any errors during the first stage, we can check that the loaded embeddings are correct. You can call the function *check_embeddings*, implemented below, which runs 3 tests:\n",
    "1. Find the most similar word for provided \"positive\" and \"negative\" words.\n",
    "2. Find which word from the given list doesn’t go with the others.\n",
    "3. Find the most similar word for the provided one.\n",
    "\n",
    "In the right case the function will return the string *These embeddings look good*. Othervise, you need to validate the previous steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_embeddings(embeddings):\n",
    "    error_text = \"Something wrong with your embeddings ('%s test isn't correct).\"\n",
    "    most_similar = embeddings.most_similar(positive=['woman', 'king'], negative=['man'])\n",
    "    if len(most_similar) < 1 or most_similar[0][0] != 'queen':\n",
    "        return error_text % \"Most similar\"\n",
    "\n",
    "    doesnt_match = embeddings.doesnt_match(['breakfast', 'cereal', 'dinner', 'lunch'])\n",
    "    if doesnt_match != 'cereal':\n",
    "        return error_text % \"Doesn't match\"\n",
    "    \n",
    "    most_similar_to_given = embeddings.most_similar_to_given('music', ['water', 'sound', 'backpack', 'mouse'])\n",
    "    if most_similar_to_given != 'sound':\n",
    "        return error_text % \"Most similar to given\"\n",
    "    \n",
    "    return \"These embeddings look good.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These embeddings look good.\n"
     ]
    }
   ],
   "source": [
    "print(check_embeddings(wv_embeddings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From word to text embeddings\n",
    "\n",
    "**Task 1 (Question2Vec).** Usually, we have word-based embeddings, but for the task we need to create a representation for the whole question. It could be done in different ways. In our case we will use a **mean** of all word vectors in the question. Now you need to implement the function *question_to_vec*, which calculates the question representation described above. This function should work with the input text as is without any preprocessing.\n",
    "\n",
    "Note that there could be words without the corresponding embeddings. In this case, you can just skip these words and don't take them into account during calculating the result. If the question doesn't contain any known word with embedding, the function should return a zero vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_to_vec(question, embeddings, dim=300):\n",
    "    \"\"\"\n",
    "        question: a string\n",
    "        embeddings: dict where the key is a word and a value is its' embedding\n",
    "        dim: size of the representation\n",
    "\n",
    "        result: vector representation for the question\n",
    "    \"\"\"\n",
    "    q_2_vec = np.zeros(dim)\n",
    "    k = 0\n",
    "    for word in question.split(\" \"):\n",
    "        if word in embeddings:\n",
    "            q_2_vec += embeddings[word]\n",
    "            k += 1\n",
    "    k = k if k else 1\n",
    "    return q_2_vec / k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check the basic correctness of your implementation, run the function *question_to_vec_tests*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_to_vec_tests():\n",
    "    if (np.zeros(300) != question_to_vec('', wv_embeddings)).any():\n",
    "        return \"You need to return zero vector for empty question.\"\n",
    "    if (np.zeros(300) != question_to_vec('thereisnosuchword', wv_embeddings)).any():\n",
    "        return \"You need to return zero vector for the question, which consists only unknown words.\"\n",
    "    if (wv_embeddings['word'] != question_to_vec('word', wv_embeddings)).any():\n",
    "        return \"You need to check the corectness of your function.\"\n",
    "    if ((wv_embeddings['I'] + wv_embeddings['am']) / 2 != question_to_vec('I am', wv_embeddings)).any():\n",
    "        return \"Your function should calculate a mean of word vectors.\"\n",
    "    if (wv_embeddings['word'] != question_to_vec('thereisnosuchword word', wv_embeddings)).any():\n",
    "        return \"You should not consider words which embeddings are unknown.\"\n",
    "    return \"Basic tests are passed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic tests are passed.\n"
     ]
    }
   ],
   "source": [
    "print(question_to_vec_tests())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can submit embeddings for the questions from the file *test_embeddings.tsv* to earn the points. In this task you don't need to transform the text of a question somehow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from util import array_to_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current answer for task Question2Vec is: 0.019293891059\n",
      "-0.0287272135417\n",
      "0.0460561116536\n",
      "0.0852593315972\n",
      "0.0243055555556\n",
      "-0.0729031032986\n",
      "0.0...\n"
     ]
    }
   ],
   "source": [
    "question2vec_result = []\n",
    "for question in open('data/test_embeddings.tsv'):\n",
    "    question = question.strip()\n",
    "    answer = question_to_vec(question, wv_embeddings)\n",
    "    question2vec_result = np.append(question2vec_result, answer)\n",
    "\n",
    "grader.submit_tag('Question2Vec', array_to_string(question2vec_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a method to create a representation of any sentence and we are ready for the first evaluation. So, let's check how well our solution (Google's vectors + *question_to_vec*) will work.\n",
    "\n",
    "## Evaluation of text similarity\n",
    "\n",
    "We can imagine that if we use good embeddings, the cosine similarity between the duplicate sentences should be less than for the random ones. Overall, for each pair of duplicate sentences we can generate *R* random negative examples and find out the position of the correct duplicate.  \n",
    "\n",
    "For example, we have the question *\"Exceptions What really happens\"* and we are sure that another question *\"How does the catch keyword determine the type of exception that was thrown\"* is a duplicate. But our model doesn't know it and tries to find out the best option also among questions like *\"How Can I Make These Links Rotate in PHP\"*, *\"NSLog array description not memory address\"* and *\"PECL_HTTP not recognised php ubuntu\"*. The goal of the model is to rank all these 4 questions (1 *positive* and *R* = 3 *negative*) in the way that the correct one is in the first place.\n",
    "\n",
    "However, it is unnatural to count on that the best candidate will be always in the first place. So let us consider the place of the best candidate in the sorted list of candidates and formulate a metric based on it. We can fix some *K* — a reasonalble number of top-ranked elements and *N* — a number of queries (size of the sample).\n",
    "\n",
    "### Hits@K\n",
    "\n",
    "The first simple metric will be a number of correct hits for some *K*:\n",
    "$$ \\text{Hits@K} = \\frac{1}{N}\\sum_{i=1}^N \\, [dup_i \\in topK(q_i)]$$\n",
    "\n",
    "where $q_i$ is the i-th query, $dup_i$ is its duplicate, $topK(q_i)$ is the top K elements of the ranked sentences provided by our model and the operation $[dup_i \\in topK(q_i)]$ equals 1 if the condition is true and 0 otherwise (more details about this operation could be found [here](https://en.wikipedia.org/wiki/Iverson_bracket)).\n",
    "\n",
    "\n",
    "### DCG@K\n",
    "The second one is a simplified [DCG metric](https://en.wikipedia.org/wiki/Discounted_cumulative_gain):\n",
    "\n",
    "$$ \\text{DCG@K} = \\frac{1}{N} \\sum_{i=1}^N\\frac{1}{\\log_2(1+rank_{dup_i})}\\cdot[rank_{dup_i} \\le K] $$\n",
    "\n",
    "where $rank_{dup_i}$ is a position of the duplicate in the sorted list of the nearest sentences for the query $q_i$. According to this metric, the model gets a higher reward for a higher position of the correct answer. If the answer does not appear in topK at all, the reward is zero. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation examples\n",
    "\n",
    "Let's calculate the described metrics for the toy example introduced above. In this case $N$ = 1 and the correct candidate for $q_1$ is *\"How does the catch keyword determine the type of exception that was thrown\"*. Consider the following ranking of the candidates:\n",
    "1. *\"How Can I Make These Links Rotate in PHP\"*\n",
    "2. *\"How does the catch keyword determine the type of exception that was thrown\"*\n",
    "3. *\"NSLog array description not memory address\"*\n",
    "4. *\"PECL_HTTP not recognised php ubuntu\"*\n",
    "\n",
    "Using the ranking above, calculate *Hits@K* metric for *K = 1, 2, 4*: \n",
    " \n",
    "- [K = 1] $\\text{Hits@1} = \\frac{1}{1}\\sum_{i=1}^1 \\, [dup_i \\in top1(q_i)] = [dup_1 \\in top1(q_1)] = 0$ because the correct answer doesn't appear in the *top1* list.\n",
    "- [K = 2] $\\text{Hits@2} = \\frac{1}{1}\\sum_{i=1}^1 \\, [dup_i \\in top2(q_i)] = [dup_1 \\in top2(q_1)] = 1$ because $rank_{dup_1} = 2$.\n",
    "- [K = 4] $\\text{Hits@4} = \\frac{1}{1}\\sum_{i=1}^1 \\, [dup_i \\in top4(q_i)] = [dup_1 \\in top4(q_1)] = 1$\n",
    "\n",
    "Using the ranking above, calculate *DCG@K* metric for *K = 1, 2, 4*:\n",
    "\n",
    "- [K = 1] $\\text{DCG@1} = \\frac{1}{1} \\sum_{i=1}^1\\frac{1}{\\log_2(1+rank_{dup_i})}\\cdot[rank_{dup_i} \\le 1] = \\frac{1}{\\log_2(1+rank_{dup_i})}\\cdot[rank_{dup_i} \\le 1] = 0$ because the correct answer doesn't appear in the top1 list.\n",
    "- [K = 2] $\\text{DCG@2} = \\frac{1}{1} \\sum_{i=1}^1\\frac{1}{\\log_2(1+rank_{dup_i})}\\cdot[rank_{dup_i} \\le 2] = \\frac{1}{\\log_2{3}}$, because $rank_{dup_1} = 2$.\n",
    "- [K = 4] $\\text{DCG@4} = \\frac{1}{1} \\sum_{i=1}^1\\frac{1}{\\log_2(1+rank_{dup_i})}\\cdot[rank_{dup_i} \\le 4] = \\frac{1}{\\log_2{3}}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tasks 2 and 3 (HitsCount and DCGScore).** Implement the functions *hits_count* and *dcg_score* as described above. Each function has two arguments: *dup_ranks* and *k*. *dup_ranks* is a list which contains *values of ranks* of duplicates. For example, *dup_ranks* is *[2]* for the example provided above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hits_count(dup_ranks, k):\n",
    "    \"\"\"\n",
    "        dup_ranks: list of duplicates' ranks; one rank per question; \n",
    "                   length is a number of questions which we are looking for duplicates; \n",
    "                   rank is a number from 1 to len(candidates of the question); \n",
    "                   e.g. [2, 3] means that the first duplicate has the rank 2, the second one — 3.\n",
    "        k: number of top-ranked elements (k in Hits@k metric)\n",
    "\n",
    "        result: return Hits@k value for current ranking\n",
    "    \"\"\"\n",
    "    hits = 0\n",
    "    for dup in dup_ranks:\n",
    "        if dup <= k:\n",
    "            hits += 1\n",
    "    return hits / len(dup_ranks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your code on the tiny examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_hits():\n",
    "    # *Evaluation example*\n",
    "    # answers — dup_i\n",
    "    answers = [\"How does the catch keyword determine the type of exception that was thrown\"]\n",
    "    \n",
    "    # candidates_ranking — the ranked sentences provided by our model\n",
    "    candidates_ranking = [[\"How Can I Make These Links Rotate in PHP\", \n",
    "                           \"How does the catch keyword determine the type of exception that was thrown\",\n",
    "                           \"NSLog array description not memory address\",\n",
    "                           \"PECL_HTTP not recognised php ubuntu\"]]\n",
    "    # dup_ranks — position of the dup_i in the list of ranks +1\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    \n",
    "    # correct_answers — the expected values of the result for each k from 1 to 4\n",
    "    correct_answers = [0, 1, 1, 1]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(hits_count(dup_ranks, k), correct):\n",
    "            return \"Check the function.\"\n",
    "    \n",
    "    # Other tests\n",
    "    answers = [\"How does the catch keyword determine the type of exception that was thrown\", \n",
    "               \"Convert Google results object (pure js) to Python object\"]\n",
    "    \n",
    "    # The first test: both duplicates on the first position in ranked list\n",
    "    candidates_ranking = [[\"How does the catch keyword determine the type of exception that was thrown\",\n",
    "                           \"How Can I Make These Links Rotate in PHP\"], \n",
    "                          [\"Convert Google results object (pure js) to Python object\",\n",
    "                           \"WPF- How to update the changes in list item of a list\"]]\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    correct_answers = [1, 1]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(hits_count(dup_ranks, k), correct):\n",
    "            return \"Check the function (test: both duplicates on the first position in ranked list).\"\n",
    "        \n",
    "    # The second test: one candidate on the first position, another — on the second\n",
    "    candidates_ranking = [[\"How Can I Make These Links Rotate in PHP\", \n",
    "                           \"How does the catch keyword determine the type of exception that was thrown\"], \n",
    "                          [\"Convert Google results object (pure js) to Python object\",\n",
    "                           \"WPF- How to update the changes in list item of a list\"]]\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    correct_answers = [0.5, 1]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(hits_count(dup_ranks, k), correct):\n",
    "            return \"Check the function (test: one candidate on the first position, another — on the second).\"\n",
    "\n",
    "    # The third test: both candidates on the second position\n",
    "    candidates_ranking = [[\"How Can I Make These Links Rotate in PHP\", \n",
    "                           \"How does the catch keyword determine the type of exception that was thrown\"], \n",
    "                          [\"WPF- How to update the changes in list item of a list\",\n",
    "                           \"Convert Google results object (pure js) to Python object\"]]\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    correct_answers = [0, 1]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(hits_count(dup_ranks, k), correct):\n",
    "            return \"Check the function (test: both candidates on the second position).\"\n",
    "\n",
    "    return \"Basic test are passed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic test are passed.\n"
     ]
    }
   ],
   "source": [
    "print(test_hits())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcg_score(dup_ranks, k):\n",
    "    \"\"\"\n",
    "        dup_ranks: list of duplicates' ranks; one rank per question; \n",
    "                   length is a number of questions which we are looking for duplicates; \n",
    "                   rank is a number from 1 to len(candidates of the question); \n",
    "                   e.g. [2, 3] means that the first duplicate has the rank 2, the second one — 3.\n",
    "        k: number of top-ranked elements (k in DCG@k metric)\n",
    "\n",
    "        result: return DCG@k value for current ranking\n",
    "    \"\"\"\n",
    "    dcg = 0\n",
    "    for dup in dup_ranks:\n",
    "        dcg += (dup<=k) / (np.log2(1 + dup))\n",
    "    return dcg / len(dup_ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_dcg():\n",
    "    # *Evaluation example*\n",
    "    # answers — dup_i\n",
    "    answers = [\"How does the catch keyword determine the type of exception that was thrown\"]\n",
    "    \n",
    "    # candidates_ranking — the ranked sentences provided by our model\n",
    "    candidates_ranking = [[\"How Can I Make These Links Rotate in PHP\", \n",
    "                           \"How does the catch keyword determine the type of exception that was thrown\",\n",
    "                           \"NSLog array description not memory address\",\n",
    "                           \"PECL_HTTP not recognised php ubuntu\"]]\n",
    "    # dup_ranks — position of the dup_i in the list of ranks +1\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    \n",
    "    # correct_answers — the expected values of the result for each k from 1 to 4\n",
    "    correct_answers = [0, 1 / (np.log2(3)), 1 / (np.log2(3)), 1 / (np.log2(3))]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(dcg_score(dup_ranks, k), correct):\n",
    "            return \"Check the function.\"\n",
    "    \n",
    "    # Other tests\n",
    "    answers = [\"How does the catch keyword determine the type of exception that was thrown\", \n",
    "               \"Convert Google results object (pure js) to Python object\"]\n",
    "\n",
    "    # The first test: both duplicates on the first position in ranked list\n",
    "    candidates_ranking = [[\"How does the catch keyword determine the type of exception that was thrown\",\n",
    "                           \"How Can I Make These Links Rotate in PHP\"], \n",
    "                          [\"Convert Google results object (pure js) to Python object\",\n",
    "                           \"WPF- How to update the changes in list item of a list\"]]\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    correct_answers = [1, 1]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(dcg_score(dup_ranks, k), correct):\n",
    "            return \"Check the function (test: both duplicates on the first position in ranked list).\"\n",
    "        \n",
    "    # The second test: one candidate on the first position, another — on the second\n",
    "    candidates_ranking = [[\"How Can I Make These Links Rotate in PHP\", \n",
    "                           \"How does the catch keyword determine the type of exception that was thrown\"], \n",
    "                          [\"Convert Google results object (pure js) to Python object\",\n",
    "                           \"WPF- How to update the changes in list item of a list\"]]\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    correct_answers = [0.5, (1 + (1 / (np.log2(3)))) / 2]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(dcg_score(dup_ranks, k), correct):\n",
    "            return \"Check the function (test: one candidate on the first position, another — on the second).\"\n",
    "        \n",
    "    # The third test: both candidates on the second position\n",
    "    candidates_ranking = [[\"How Can I Make These Links Rotate in PHP\",\n",
    "                           \"How does the catch keyword determine the type of exception that was thrown\"], \n",
    "                          [\"WPF- How to update the changes in list item of a list\",\n",
    "                           \"Convert Google results object (pure js) to Python object\"]]\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    correct_answers = [0, 1 / (np.log2(3))]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(dcg_score(dup_ranks, k), correct):\n",
    "            return \"Check the function (test: both candidates on the second position).\"\n",
    "\n",
    "    return \"Basic test are passed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic test are passed.\n"
     ]
    }
   ],
   "source": [
    "print(test_dcg())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit results of the functions *hits_count* and *dcg_score* for the following examples to earn the points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_examples = [\n",
    "    [1],\n",
    "    [1, 2],\n",
    "    [2, 1],\n",
    "    [1, 2, 3],\n",
    "    [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    [9, 5, 4, 2, 8, 10, 7, 6, 1, 3],\n",
    "    [4, 3, 5, 1, 9, 10, 7, 8, 2, 6],\n",
    "    [5, 1, 7, 6, 2, 3, 8, 9, 10, 4],\n",
    "    [6, 3, 1, 4, 7, 2, 9, 8, 10, 5],\n",
    "    [10, 9, 8, 7, 6, 5, 4, 3, 2, 1],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current answer for task HitsCount is: 1.0\n",
      "0.5\n",
      "1.0\n",
      "0.5\n",
      "1.0\n",
      "0.3333333333333333\n",
      "0.6666666666666666\n",
      "1.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "1....\n"
     ]
    }
   ],
   "source": [
    "hits_results = []\n",
    "for example in test_examples:\n",
    "    for k in range(len(example)):\n",
    "        hits_results.append(hits_count(example, k + 1))\n",
    "grader.submit_tag('HitsCount', array_to_string(hits_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current answer for task DCGScore is: 1.0\n",
      "0.5\n",
      "0.815464876786\n",
      "0.5\n",
      "0.815464876786\n",
      "0.333333333333\n",
      "0.54364325119\n",
      "0.710309917857\n",
      "0.1\n",
      "0.16309297...\n"
     ]
    }
   ],
   "source": [
    "dcg_results = []\n",
    "for example in test_examples:\n",
    "    for k in range(len(example)):\n",
    "        dcg_results.append(dcg_score(example, k + 1))\n",
    "grader.submit_tag('DCGScore', array_to_string(dcg_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  First solution: pre-trained embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will work with predefined train, validation and test corpora. All the files are tab-separated, but have a different format:\n",
    " - *train* corpus contains similar sentences at the same row.\n",
    " - *validation* corpus contains the following columns: *question*, *similar question*, *negative example 1*, *negative example 2*, ... \n",
    " - *test* corpus contains the following columns: *question*, *example 1*, *example 2*, ...\n",
    "\n",
    "Validation corpus will be used for the intermediate validation of models. The test data will be necessary for submitting the quality of your model in the system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should upload *validation* corpus to evaluate current solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(filename):\n",
    "    data = []\n",
    "    for line in open(filename, encoding='utf-8'):\n",
    "        data.append(line.strip().split('\\t'))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename =  \"data\" + \"/\" + \"validation.tsv\"\n",
    "validation = read_corpus(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use cosine distance to rank candidate questions which you need to implement in the function *rank_candidates*. The function should return a sorted list of pairs *(initial position in candidates list, candidate)*. Index of some pair corresponds to its rank (the first is the best). For example, if the list of candidates was *[a, b, c]* and the most similar is *c*, then *a* and *b*, the function should return a list *[(2, c), (0, a), (1, b)]*.\n",
    "\n",
    "Pay attention, if you use the function *cosine_similarity* from *sklearn.metrics.pairwise* to calculate similarity because it works in a different way: most similar objects has greatest similarity. It's preferable to use a vectorized version of *cosine_similarity* function. Try to compute similarity at once and not use list comprehension. It should speed up your computations significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_candidates(question, candidates, embeddings, dim=300):\n",
    "    \"\"\"\n",
    "        question: a string\n",
    "        candidates: a list of strings (candidates) which we want to rank\n",
    "        embeddings: some embeddings\n",
    "        dim: dimension of the current embeddings\n",
    "        \n",
    "        result: a list of pairs (initial position in the list, question)\n",
    "    \"\"\"\n",
    "    q_2_v = question_to_vec(question=question, dim=dim, embeddings=embeddings)\n",
    "    c_2_v = [question_to_vec(question=c, dim=dim, embeddings=embeddings) for c in candidates]\n",
    "    distances = cosine_similarity([q_2_v] + c_2_v)\n",
    "    return sorted(enumerate(candidates), key=lambda x: -distances[0][1:][x[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your code on the tiny examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_rank_candidates():\n",
    "    questions = ['converting string to list', 'Sending array via Ajax fails']\n",
    "    candidates = [['Convert Google results object (pure js) to Python object', \n",
    "                   'C# create cookie from string and send it',\n",
    "                   'How to use jQuery AJAX for an outside domain?'], \n",
    "                  ['Getting all list items of an unordered list in PHP', \n",
    "                   'WPF- How to update the changes in list item of a list', \n",
    "                   'select2 not displaying search results']]\n",
    "    results = [[(1, 'C# create cookie from string and send it'), \n",
    "                (0, 'Convert Google results object (pure js) to Python object'), \n",
    "                (2, 'How to use jQuery AJAX for an outside domain?')],\n",
    "               [(0, 'Getting all list items of an unordered list in PHP'), \n",
    "                (2, 'select2 not displaying search results'), \n",
    "                (1, 'WPF- How to update the changes in list item of a list')]]\n",
    "    for question, q_candidates, result in zip(questions, candidates, results):\n",
    "        ranks = rank_candidates(question, q_candidates, wv_embeddings, 300)\n",
    "        if not np.all(ranks == result):\n",
    "            return \"Check the function.\"\n",
    "    return \"Basic tests are passed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic tests are passed.\n"
     ]
    }
   ],
   "source": [
    "print(test_rank_candidates())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can test the quality of the current approach. Run the next two cells to get the results. Pay attention that calculation of similarity between vectors takes time and this calculation is computed approximately in 10 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_ranking = []\n",
    "for line in validation:\n",
    "    q, *ex = line\n",
    "    ranks = rank_candidates(q, ex, wv_embeddings)\n",
    "    wv_ranking.append([r[0] for r in ranks].index(0) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCG@   1: 0.212 | Hits@   1: 0.212\n",
      "DCG@   5: 0.267 | Hits@   5: 0.315\n",
      "DCG@  10: 0.282 | Hits@  10: 0.363\n",
      "DCG@ 100: 0.320 | Hits@ 100: 0.552\n",
      "DCG@ 500: 0.353 | Hits@ 500: 0.811\n",
      "DCG@1000: 0.373 | Hits@1000: 1.000\n"
     ]
    }
   ],
   "source": [
    "for k in [1, 5, 10, 100, 500, 1000]:\n",
    "    print(\"DCG@%4d: %.3f | Hits@%4d: %.3f\" % (k, dcg_score(wv_ranking, k), k, hits_count(wv_ranking, k)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you did all the steps correctly, you should be frustrated by the received results. Let's try to understand why the quality is so low. First of all, when you work with some data it is necessary to have an idea how the data looks like. Print several questions from the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How to print a binary heap tree without recursion? How do you best convert a recursive function to an iterative one? How can i use ng-model with directive in angular js flash: drawing and erasing\n",
      "How to start PhoneStateListener programmatically? PhoneStateListener and service Java cast object[] to model WCF and What does this mean?\n",
      "jQuery: Show a div2 when mousenter over div1 is over when hover on div1 depenting on if it is on div2 or not it should act differently How to run selenium in google app engine/cloud? Python Comparing two lists of strings for similarities\n"
     ]
    }
   ],
   "source": [
    "for line in validation[:3]:\n",
    "    q, *examples = line\n",
    "    print(q, *examples[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we deal with the raw data. It means that we have many punctuation marks, special characters and unlowercased letters. In our case, it could lead to the situation where we can't find some embeddings, e.g. for the word \"grid?\". \n",
    "\n",
    "To solve this problem you should use the functions *text_prepare* from the previous assignments to prepare the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import text_prepare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now transform all the questions from the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_validation = []\n",
    "for line in validation:\n",
    "    line = [text_prepare(el) for el in line]\n",
    "    prepared_validation.append(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate the approach again after the preparation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_prepared_ranking = []\n",
    "for line in prepared_validation:\n",
    "    q, *ex = line\n",
    "    ranks = rank_candidates(q, ex, wv_embeddings)\n",
    "    wv_prepared_ranking.append([r[0] for r in ranks].index(0) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCG@   1: 0.310 | Hits@   1: 0.310\n",
      "DCG@   5: 0.380 | Hits@   5: 0.443\n",
      "DCG@  10: 0.397 | Hits@  10: 0.494\n",
      "DCG@ 100: 0.430 | Hits@ 100: 0.661\n",
      "DCG@ 500: 0.453 | Hits@ 500: 0.835\n",
      "DCG@1000: 0.470 | Hits@1000: 1.000\n"
     ]
    }
   ],
   "source": [
    "for k in [1, 5, 10, 100, 500, 1000]:\n",
    "    print(\"DCG@%4d: %.3f | Hits@%4d: %.3f\" % (k, dcg_score(wv_prepared_ranking, k), \n",
    "                                              k, hits_count(wv_prepared_ranking, k)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, prepare also train and test data, because you will need it in the future:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_file(in_, out_):\n",
    "    out = open(out_, 'w')\n",
    "    for line in open(in_, encoding='utf8'):\n",
    "        line = line.strip().split('\\t')\n",
    "        new_line = [text_prepare(q) for q in line]\n",
    "        print(*new_line, sep='\\t', file=out)\n",
    "    out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_file(\"data/train.tsv\", \"data/prepared_train.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_file(\"data/test.tsv\", \"data/prepared_test.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4 (W2VTokenizedRanks).** For each question from prepared *test.tsv* submit the ranks of the candidates to earn the points. The calculations should take about 3-5 minutes. Pay attention that the function *rank_candidates* returns a ranking, while in this case you should find a position in this ranking. Ranks should start with 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import matrix_to_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current answer for task W2VTokenizedRanks is: 95\t94\t7\t9\t64\t36\t31\t93\t23\t100\t99\t20\t60\t6\t97\t48\t70\t37\t41\t96\t29\t56\t2\t65\t68\t44\t27\t25\t57\t62\t11\t87\t50\t66\t7...\n"
     ]
    }
   ],
   "source": [
    "w2v_ranks_results = []\n",
    "prepared_test_data = \"data/prepared_test.tsv\"\n",
    "for line in open(prepared_test_data):\n",
    "    q, *ex = line.strip().split('\\t')\n",
    "    ranks = rank_candidates(q, ex, wv_embeddings, 300)\n",
    "    ranked_candidates = [r[0] for r in ranks]\n",
    "    w2v_ranks_results.append([ranked_candidates.index(i) + 1 for i in range(len(ranked_candidates))])\n",
    "    \n",
    "grader.submit_tag('W2VTokenizedRanks', matrix_to_string(w2v_ranks_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced solution: StarSpace embeddings\n",
    "\n",
    "Now you are ready to train your own word embeddings! In particular, you need to train embeddings specially for our task of duplicates detection. Unfortunately, StarSpace cannot be run on Windows and we recommend to use provided\n",
    "[docker container](https://github.com/hse-aml/natural-language-processing/blob/master/Docker-tutorial.md) or other alternatives. Don't delete results of this task because you will need it in the final project.\n",
    "\n",
    "### How it works and what's the main difference with word2vec?\n",
    "The main point in this section is that StarSpace can be trained specifically for some tasks. In contrast to word2vec model, which tries to train similar embeddings for words in similar contexts, StarSpace uses embeddings for the whole sentence (just as a sum of embeddings of words and phrases). Despite the fact that in both cases we get word embeddings as a result of the training, StarSpace embeddings are trained using some supervised data, e.g. a set of similar sentence pairs, and thus they can better suit the task.\n",
    "\n",
    "In our case, StarSpace should use two types of sentence pairs for training: \"positive\" and \"negative\". \"Positive\" examples are extracted from the train sample (duplicates, high similarity) and the \"negative\" examples are generated randomly (low similarity assumed). \n",
    "\n",
    "### How to choose the best params for the model?\n",
    "Normally, you would start with some default choice and then run extensive experiments to compare different strategies. However, we have some recommendations ready for you to save your time:\n",
    "- Be careful with choosing the suitable training mode. In this task we want to explore texts similarity which corresponds to *trainMode = 3*.\n",
    "- Use adagrad optimization (parameter *adagrad = true*).\n",
    "- Set the length of phrase equal to 1 (parameter *ngrams*), because we need embeddings only for words.\n",
    "- Don't use a large number of *epochs* (we think that 5 should be enough).\n",
    "- Try dimension *dim* equal to 100.\n",
    "- To compare embeddings usually *cosine* *similarity* is used.\n",
    "- Set *minCount* greater than 1 (for example, 2) if you don't want to get embeddings for extremely rare words.\n",
    "- Parameter *verbose = true* could show you the progress of the training process.\n",
    "- Set parameter *fileFormat* equals *labelDoc*.\n",
    "- Parameter *negSearchLimit* is responsible for a number of negative examples which is used during the training. We think that 10 will be enought for this task.\n",
    "- To increase a speed of training we recommend to set *learning rate* to 0.05."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train StarSpace embeddings for unigrams on the train dataset. You don't need to change the format of the input data. Just don't forget to use prepared version of the training data. \n",
    "\n",
    "If you follow the instruction, the training process will take about 1 hour. The size of the embeddings' dictionary should be approximately 100 000 (number of lines in the result file). If you got significantly more than this number, try to check all the instructions above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arguments: \n",
      "lr: 0.05\n",
      "dim: 100\n",
      "epoch: 5\n",
      "maxTrainTime: 8640000\n",
      "saveEveryEpoch: 0\n",
      "loss: hinge\n",
      "margin: 0.05\n",
      "similarity: cosine\n",
      "maxNegSamples: 10\n",
      "negSearchLimit: 10\n",
      "thread: 20\n",
      "minCount: 5\n",
      "minCountLabel: 1\n",
      "label: __label__\n",
      "ngrams: 1\n",
      "bucket: 2000000\n",
      "adagrad: 1\n",
      "trainMode: 3\n",
      "fileFormat: labelDoc\n",
      "normalizeText: 1\n",
      "dropoutLHS: 0\n",
      "dropoutRHS: 0\n",
      "Start to initialize starspace model.\n",
      "Build dict from input file : data/prepared_train.tsv\n",
      "Read 12M words\n",
      "Number of words in dictionary:  42840\n",
      "Number of labels in dictionary: 0\n",
      "Loading data from file : data/prepared_train.tsv\n",
      "Total number of examples loaded : 999408\n",
      "Initialized model weights. Model size :\n",
      "matrix : 42840 100\n",
      "Training epoch 0: 0.05 0.01\n",
      "Epoch: 100.0%  lr: 0.040010  loss: 0.008421  eta: 0h13m  tot: 0h3m15s  (20.0%)  lr: 0.049950  loss: 0.058612  eta: 0h14m  tot: 0h0m1s  (0.1%)4.8%  lr: 0.049500  loss: 0.025232  eta: 0h17m  tot: 0h0m10s  (1.0%)5.9%  lr: 0.049349  loss: 0.023617  eta: 0h17m  tot: 0h0m12s  (1.2%)6.8%  lr: 0.049229  loss: 0.022355  eta: 0h17m  tot: 0h0m14s  (1.4%)8.7%  lr: 0.048889  loss: 0.020353  eta: 0h17m  tot: 0h0m19s  (1.7%)10.3%  lr: 0.048709  loss: 0.019319  eta: 0h17m  tot: 0h0m22s  (2.1%)11.6%  lr: 0.048579  loss: 0.018650  eta: 0h17m  tot: 0h0m25s  (2.3%)11.8%  lr: 0.048579  loss: 0.018460  eta: 0h17m  tot: 0h0m25s  (2.4%)13.3%  lr: 0.048428  loss: 0.017702  eta: 0h17m  tot: 0h0m28s  (2.7%)13.5%  lr: 0.048408  loss: 0.017573  eta: 0h17m  tot: 0h0m29s  (2.7%)17m  tot: 0h0m34s  (3.3%)17.7%  lr: 0.048018  loss: 0.015838  eta: 0h16m  tot: 0h0m37s  (3.5%)%  lr: 0.047898  loss: 0.015314  eta: 0h16m  tot: 0h0m40s  (3.8%)19.4%  lr: 0.047898  loss: 0.015248  eta: 0h16m  tot: 0h0m40s  (3.9%)  lr: 0.047738  loss: 0.014854  eta: 0h16m  tot: 0h0m43s  (4.2%)  loss: 0.014785  eta: 0h16m  tot: 0h0m44s  (4.2%)%  lr: 0.047648  loss: 0.014448  eta: 0h16m  tot: 0h0m46s  (4.4%)%  lr: 0.047538  loss: 0.014208  eta: 0h16m  tot: 0h0m48s  (4.6%)23.4%  lr: 0.047518  loss: 0.014168  eta: 0h16m  tot: 0h0m48s  (4.7%)%  lr: 0.047427  loss: 0.014010  eta: 0h16m  tot: 0h0m50s  (4.8%)24.9%  lr: 0.047347  loss: 0.013819  eta: 0h16m  tot: 0h0m51s  (5.0%)25.5%  lr: 0.047257  loss: 0.013693  eta: 0h16m  tot: 0h0m53s  (5.1%)25.9%  lr: 0.047227  loss: 0.013598  eta: 0h16m  tot: 0h0m53s  (5.2%)26.6%  lr: 0.047147  loss: 0.013487  eta: 0h16m  tot: 0h0m55s  (5.3%)29.8%  lr: 0.046797  loss: 0.012850  eta: 0h16m  tot: 0h1m1s  (6.0%)30.0%  lr: 0.046757  loss: 0.012804  eta: 0h16m  tot: 0h1m1s  (6.0%)  eta: 0h15m  tot: 0h1m3s  (6.2%)31.7%  lr: 0.046577  loss: 0.012497  eta: 0h15m  tot: 0h1m5s  (6.3%)33.1%  lr: 0.046497  loss: 0.012375  eta: 0h15m  tot: 0h1m7s  (6.6%)%  lr: 0.046306  loss: 0.012108  eta: 0h15m  tot: 0h1m11s  (7.0%)36.1%  lr: 0.046196  loss: 0.011970  eta: 0h15m  tot: 0h1m13s  (7.2%)m14s  (7.3%)36.5%  lr: 0.046156  loss: 0.011952  eta: 0h15m  tot: 0h1m14s  (7.3%)%  lr: 0.045966  loss: 0.011775  eta: 0h15m  tot: 0h1m18s  (7.6%)39.4%  lr: 0.045866  loss: 0.011650  eta: 0h15m  tot: 0h1m20s  (7.9%)39.9%  lr: 0.045816  loss: 0.011600  eta: 0h15m  tot: 0h1m21s  (8.0%)40.9%  lr: 0.045756  loss: 0.011515  eta: 0h15m  tot: 0h1m23s  (8.2%)41.4%  lr: 0.045726  loss: 0.011467  eta: 0h15m  tot: 0h1m24s  (8.3%)42.4%  lr: 0.045566  loss: 0.011375  eta: 0h15m  tot: 0h1m26s  (8.5%)%  lr: 0.045385  loss: 0.011191  eta: 0h15m  tot: 0h1m29s  (8.8%)45.2%  lr: 0.045275  loss: 0.011056  eta: 0h15m  tot: 0h1m32s  (9.0%)46.0%  lr: 0.045165  loss: 0.010997  eta: 0h15m  tot: 0h1m33s  (9.2%)46.2%  lr: 0.045155  loss: 0.010985  eta: 0h15m  tot: 0h1m33s  (9.2%)46.8%  lr: 0.045095  loss: 0.010957  eta: 0h15m  tot: 0h1m34s  (9.4%)47.0%  lr: 0.045075  loss: 0.010936  eta: 0h15m  tot: 0h1m35s  (9.4%)47.7%  lr: 0.044975  loss: 0.010875  eta: 0h15m  tot: 0h1m36s  (9.5%)49.0%  lr: 0.044905  loss: 0.010762  eta: 0h15m  tot: 0h1m39s  (9.8%)49.6%  lr: 0.044855  loss: 0.010688  eta: 0h15m  tot: 0h1m40s  (9.9%)49.8%  lr: 0.044845  loss: 0.010692  eta: 0h15m  tot: 0h1m40s  (10.0%)50.6%  lr: 0.044795  loss: 0.010623  eta: 0h15m  tot: 0h1m42s  (10.1%)0.010572  eta: 0h15m  tot: 0h1m43s  (10.3%)56.1%  lr: 0.044264  loss: 0.010242  eta: 0h14m  tot: 0h1m52s  (11.2%)56.7%  lr: 0.044154  loss: 0.010209  eta: 0h14m  tot: 0h1m54s  (11.3%)57.4%  lr: 0.044094  loss: 0.010168  eta: 0h14m  tot: 0h1m55s  (11.5%)  lr: 0.043934  loss: 0.010044  eta: 0h14m  tot: 0h1m59s  (11.8%)62.7%  lr: 0.043554  loss: 0.009841  eta: 0h14m  tot: 0h2m6s  (12.5%)64.8%  lr: 0.043293  loss: 0.009731  eta: 0h14m  tot: 0h2m10s  (13.0%)65.2%  lr: 0.043263  loss: 0.009700  eta: 0h14m  tot: 0h2m11s  (13.0%)66.7%  lr: 0.043093  loss: 0.009630  eta: 0h14m  tot: 0h2m14s  (13.3%)68.2%  lr: 0.042993  loss: 0.009563  eta: 0h14m  tot: 0h2m17s  (13.6%)68.8%  lr: 0.042943  loss: 0.009536  eta: 0h14m  tot: 0h2m18s  (13.8%)70.5%  lr: 0.042763  loss: 0.009471  eta: 0h14m  tot: 0h2m21s  (14.1%)71.9%  lr: 0.042653  loss: 0.009413  eta: 0h14m  tot: 0h2m24s  (14.4%)72.6%  lr: 0.042573  loss: 0.009369  eta: 0h14m  tot: 0h2m25s  (14.5%)72.8%  lr: 0.042533  loss: 0.009363  eta: 0h14m  tot: 0h2m26s  (14.6%)74.3%  lr: 0.042433  loss: 0.009297  eta: 0h14m  tot: 0h2m29s  (14.9%)78.7%  lr: 0.041962  loss: 0.009120  eta: 0h14m  tot: 0h2m38s  (15.7%)79.1%  lr: 0.041942  loss: 0.009108  eta: 0h14m  tot: 0h2m39s  (15.8%)81.6%  lr: 0.041662  loss: 0.008996  eta: 0h13m  tot: 0h2m43s  (16.3%)81.7%  lr: 0.041652  loss: 0.008999  eta: 0h13m  tot: 0h2m44s  (16.3%)  lr: 0.041562  loss: 0.008959  eta: 0h13m  tot: 0h2m45s  (16.5%)85.0%  lr: 0.041361  loss: 0.008876  eta: 0h13m  tot: 0h2m50s  (17.0%)85.2%  lr: 0.041331  loss: 0.008872  eta: 0h13m  tot: 0h2m51s  (17.0%)88.2%  lr: 0.041021  loss: 0.008774  eta: 0h13m  tot: 0h2m56s  (17.6%)88.6%  lr: 0.040991  loss: 0.008758  eta: 0h13m  tot: 0h2m57s  (17.7%)%  lr: 0.040971  loss: 0.008750  eta: 0h13m  tot: 0h2m57s  (17.8%)89.5%  lr: 0.040901  loss: 0.008715  eta: 0h13m  tot: 0h2m59s  (17.9%)89.9%  lr: 0.040841  loss: 0.008697  eta: 0h13m  tot: 0h3m0s  (18.0%)90.5%  lr: 0.040831  loss: 0.008670  eta: 0h13m  tot: 0h3m1s  (18.1%)91.6%  lr: 0.040681  loss: 0.008641  eta: 0h13m  tot: 0h3m3s  (18.3%)92.6%  lr: 0.040601  loss: 0.008597  eta: 0h13m  tot: 0h3m5s  (18.5%)%  lr: 0.040411  loss: 0.008583  eta: 0h13m  tot: 0h3m7s  (18.8%)%  lr: 0.040371  loss: 0.008574  eta: 0h13m  tot: 0h3m8s  (18.9%)94.7%  lr: 0.040340  loss: 0.008561  eta: 0h13m  tot: 0h3m9s  (18.9%)0.040330  loss: 0.008553  eta: 0h13m  tot: 0h3m9s  (19.0%)96.6%  lr: 0.040180  loss: 0.008503  eta: 0h13m  tot: 0h3m12s  (19.3%)96.8%  lr: 0.040170  loss: 0.008506  eta: 0h13m  tot: 0h3m13s  (19.4%)\n",
      " ---+++                Epoch    0 Train error : 0.00833714 +++--- ���\n",
      "Training epoch 1: 0.04 0.01\n",
      "Epoch: 100.0%  lr: 0.030000  loss: 0.002863  eta: 0h9m  tot: 0h6m20s  (40.0%).5%  lr: 0.039760  loss: 0.002788  eta: 0h13m  tot: 0h3m20s  (20.5%)4.2%  lr: 0.039570  loss: 0.002830  eta: 0h13m  tot: 0h3m23s  (20.8%)4.9%  lr: 0.039459  loss: 0.002780  eta: 0h12m  tot: 0h3m25s  (21.0%)5.3%  lr: 0.039409  loss: 0.002821  eta: 0h12m  tot: 0h3m25s  (21.1%)7.2%  lr: 0.039199  loss: 0.002835  eta: 0h12m  tot: 0h3m29s  (21.4%)7.6%  lr: 0.039149  loss: 0.002878  eta: 0h12m  tot: 0h3m29s  (21.5%)0.002750  eta: 0h12m  tot: 0h3m33s  (21.9%)14.1%  lr: 0.038378  loss: 0.002686  eta: 0h11m  tot: 0h3m41s  (22.8%)19.2%  lr: 0.037828  loss: 0.002707  eta: 0h11m  tot: 0h3m50s  (23.8%)20.3%  lr: 0.037678  loss: 0.002752  eta: 0h11m  tot: 0h3m53s  (24.1%)26.6%  lr: 0.037117  loss: 0.002760  eta: 0h11m  tot: 0h4m3s  (25.3%)26.8%  lr: 0.037107  loss: 0.002750  eta: 0h11m  tot: 0h4m3s  (25.4%)27.2%  lr: 0.037057  loss: 0.002769  eta: 0h11m  tot: 0h4m4s  (25.4%)27.4%  lr: 0.037047  loss: 0.002772  eta: 0h11m  tot: 0h4m4s  (25.5%)0.002759  eta: 0h11m  tot: 0h4m7s  (25.7%)29.7%  lr: 0.036797  loss: 0.002786  eta: 0h11m  tot: 0h4m8s  (25.9%)29.8%  lr: 0.036757  loss: 0.002794  eta: 0h11m  tot: 0h4m9s  (26.0%)%  lr: 0.036707  loss: 0.002817  eta: 0h11m  tot: 0h4m10s  (26.1%)31.6%  lr: 0.036527  loss: 0.002824  eta: 0h11m  tot: 0h4m12s  (26.3%)%  lr: 0.036467  loss: 0.002808  eta: 0h11m  tot: 0h4m14s  (26.5%)33.5%  lr: 0.036406  loss: 0.002803  eta: 0h11m  tot: 0h4m16s  (26.7%)33.6%  lr: 0.036356  loss: 0.002800  eta: 0h11m  tot: 0h4m16s  (26.7%)34.0%  lr: 0.036346  loss: 0.002792  eta: 0h11m  tot: 0h4m17s  (26.8%)35.7%  lr: 0.036166  loss: 0.002822  eta: 0h11m  tot: 0h4m20s  (27.1%)37.8%  lr: 0.035956  loss: 0.002824  eta: 0h11m  tot: 0h4m24s  (27.6%)39.2%  lr: 0.035836  loss: 0.002830  eta: 0h11m  tot: 0h4m27s  (27.8%)39.5%  lr: 0.035836  loss: 0.002846  eta: 0h11m  tot: 0h4m27s  (27.9%)39.7%  lr: 0.035806  loss: 0.002863  eta: 0h11m  tot: 0h4m28s  (27.9%)40.7%  lr: 0.035696  loss: 0.002837  eta: 0h10m  tot: 0h4m30s  (28.1%)0.002835  eta: 0h10m  tot: 0h4m32s  (28.4%)42.2%  lr: 0.035526  loss: 0.002832  eta: 0h10m  tot: 0h4m32s  (28.4%)  lr: 0.035506  loss: 0.002830  eta: 0h10m  tot: 0h4m33s  (28.5%)43.5%  lr: 0.035476  loss: 0.002825  eta: 0h10m  tot: 0h4m35s  (28.7%)0.002805  eta: 0h10m  tot: 0h4m38s  (29.0%)  eta: 0h10m  tot: 0h4m38s  (29.0%)47.1%  lr: 0.035185  loss: 0.002815  eta: 0h10m  tot: 0h4m42s  (29.4%)48.1%  lr: 0.035015  loss: 0.002815  eta: 0h10m  tot: 0h4m43s  (29.6%)48.3%  lr: 0.035015  loss: 0.002826  eta: 0h10m  tot: 0h4m44s  (29.7%)49.0%  lr: 0.034905  loss: 0.002809  eta: 0h10m  tot: 0h4m45s  (29.8%)52.5%  lr: 0.034485  loss: 0.002805  eta: 0h10m  tot: 0h4m51s  (30.5%)%  lr: 0.034364  loss: 0.002812  eta: 0h10m  tot: 0h4m54s  (30.8%)55.5%  lr: 0.034264  loss: 0.002829  eta: 0h10m  tot: 0h4m57s  (31.1%)59.3%  lr: 0.033884  loss: 0.002838  eta: 0h10m  tot: 0h5m4s  (31.9%)59.9%  lr: 0.033824  loss: 0.002853  eta: 0h10m  tot: 0h5m5s  (32.0%)60.5%  lr: 0.033804  loss: 0.002852  eta: 0h10m  tot: 0h5m7s  (32.1%)62.9%  lr: 0.033554  loss: 0.002856  eta: 0h10m  tot: 0h5m11s  (32.6%)64.1%  lr: 0.033494  loss: 0.002867  eta: 0h10m  tot: 0h5m13s  (32.8%)64.6%  lr: 0.033444  loss: 0.002863  eta: 0h10m  tot: 0h5m14s  (32.9%)65.4%  lr: 0.033393  loss: 0.002873  eta: 0h10m  tot: 0h5m16s  (33.1%)0.033293  loss: 0.002869  eta: 0h10m  tot: 0h5m18s  (33.3%)%  lr: 0.033283  loss: 0.002869  eta: 0h10m  tot: 0h5m19s  (33.4%)0.002878  eta: 0h10m  tot: 0h5m22s  (33.7%)69.4%  lr: 0.033023  loss: 0.002872  eta: 0h10m  tot: 0h5m23s  (33.9%)70.5%  lr: 0.032853  loss: 0.002867  eta: 0h10m  tot: 0h5m26s  (34.1%)71.7%  lr: 0.032753  loss: 0.002854  eta: 0h10m  tot: 0h5m28s  (34.3%)74.9%  lr: 0.032453  loss: 0.002847  eta: 0h10m  tot: 0h5m34s  (35.0%)75.7%  lr: 0.032403  loss: 0.002839  eta: 0h10m  tot: 0h5m35s  (35.1%)10m  tot: 0h5m37s  (35.2%)76.4%  lr: 0.032332  loss: 0.002832  eta: 0h10m  tot: 0h5m37s  (35.3%)78.1%  lr: 0.032202  loss: 0.002845  eta: 0h9m  tot: 0h5m40s  (35.6%)79.7%  lr: 0.031982  loss: 0.002856  eta: 0h9m  tot: 0h5m43s  (35.9%)%  lr: 0.031832  loss: 0.002855  eta: 0h9m  tot: 0h5m45s  (36.2%)82.1%  lr: 0.031672  loss: 0.002846  eta: 0h9m  tot: 0h5m48s  (36.4%)%  lr: 0.031592  loss: 0.002854  eta: 0h9m  tot: 0h5m50s  (36.6%)83.8%  lr: 0.031532  loss: 0.002850  eta: 0h9m  tot: 0h5m51s  (36.8%)0.002846  eta: 0h9m  tot: 0h5m53s  (37.0%)87.3%  lr: 0.031261  loss: 0.002846  eta: 0h9m  tot: 0h5m57s  (37.5%)87.8%  lr: 0.031221  loss: 0.002844  eta: 0h9m  tot: 0h5m59s  (37.6%)88.6%  lr: 0.031151  loss: 0.002847  eta: 0h9m  tot: 0h6m0s  (37.7%)89.0%  lr: 0.031141  loss: 0.002850  eta: 0h9m  tot: 0h6m1s  (37.8%)90.1%  lr: 0.031031  loss: 0.002852  eta: 0h9m  tot: 0h6m3s  (38.0%)90.5%  lr: 0.031001  loss: 0.002852  eta: 0h9m  tot: 0h6m4s  (38.1%)90.7%  lr: 0.030991  loss: 0.002852  eta: 0h9m  tot: 0h6m4s  (38.1%)%  lr: 0.030931  loss: 0.002858  eta: 0h9m  tot: 0h6m6s  (38.3%)%  lr: 0.030791  loss: 0.002858  eta: 0h9m  tot: 0h6m8s  (38.5%)%  lr: 0.030671  loss: 0.002856  eta: 0h9m  tot: 0h6m9s  (38.6%)95.1%  lr: 0.030441  loss: 0.002858  eta: 0h9m  tot: 0h6m13s  (39.0%)95.4%  lr: 0.030401  loss: 0.002859  eta: 0h9m  tot: 0h6m14s  (39.1%)97.1%  lr: 0.030200  loss: 0.002861  eta: 0h9m  tot: 0h6m17s  (39.4%)97.5%  lr: 0.030120  loss: 0.002861  eta: 0h9m  tot: 0h6m17s  (39.5%)97.7%  lr: 0.030110  loss: 0.002863  eta: 0h9m  tot: 0h6m18s  (39.5%)98.1%  lr: 0.030090  loss: 0.002859  eta: 0h9m  tot: 0h6m18s  (39.6%)\n",
      " ---+++                Epoch    1 Train error : 0.00295428 +++--- ���\n",
      "Training epoch 2: 0.03 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100.0%  lr: 0.020000  loss: 0.002141  eta: 0h6m  tot: 0h9m28s  (60.0%)8%  lr: 0.029680  loss: 0.001656  eta: 0h7m  tot: 0h6m26s  (40.8%)4.2%  lr: 0.029640  loss: 0.001649  eta: 0h7m  tot: 0h6m27s  (40.8%)%  lr: 0.029620  loss: 0.001669  eta: 0h7m  tot: 0h6m27s  (40.9%)5.9%  lr: 0.029510  loss: 0.001883  eta: 0h8m  tot: 0h6m30s  (41.2%)6.5%  lr: 0.029500  loss: 0.001864  eta: 0h8m  tot: 0h6m31s  (41.3%)7.2%  lr: 0.029419  loss: 0.001827  eta: 0h8m  tot: 0h6m33s  (41.4%)7.8%  lr: 0.029359  loss: 0.001813  eta: 0h8m  tot: 0h6m34s  (41.6%)%  lr: 0.029119  loss: 0.001890  eta: 0h8m  tot: 0h6m37s  (41.9%)%  lr: 0.029049  loss: 0.001862  eta: 0h8m  tot: 0h6m39s  (42.1%)11.2%  lr: 0.028989  loss: 0.001924  eta: 0h8m  tot: 0h6m40s  (42.2%)14.1%  lr: 0.028709  loss: 0.002064  eta: 0h8m  tot: 0h6m46s  (42.8%)0.002053  eta: 0h8m  tot: 0h6m47s  (42.9%)15.2%  lr: 0.028609  loss: 0.002037  eta: 0h8m  tot: 0h6m48s  (43.0%)16.0%  lr: 0.028499  loss: 0.002026  eta: 0h8m  tot: 0h6m49s  (43.2%)16.2%  lr: 0.028448  loss: 0.002035  eta: 0h8m  tot: 0h6m50s  (43.2%)17.7%  lr: 0.028198  loss: 0.002053  eta: 0h8m  tot: 0h6m52s  (43.5%)18.1%  lr: 0.028148  loss: 0.002062  eta: 0h8m  tot: 0h6m53s  (43.6%)18.4%  lr: 0.028128  loss: 0.002110  eta: 0h8m  tot: 0h6m54s  (43.7%)19.0%  lr: 0.028048  loss: 0.002127  eta: 0h8m  tot: 0h6m55s  (43.8%)19.2%  lr: 0.028038  loss: 0.002128  eta: 0h8m  tot: 0h6m55s  (43.8%)21.7%  lr: 0.027748  loss: 0.002174  eta: 0h8m  tot: 0h7m0s  (44.3%)21.9%  lr: 0.027728  loss: 0.002171  eta: 0h8m  tot: 0h7m0s  (44.4%)24.0%  lr: 0.027508  loss: 0.002192  eta: 0h8m  tot: 0h7m4s  (44.8%)26.0%  lr: 0.027247  loss: 0.002200  eta: 0h8m  tot: 0h7m8s  (45.2%)26.4%  lr: 0.027197  loss: 0.002206  eta: 0h8m  tot: 0h7m9s  (45.3%)27.2%  lr: 0.027157  loss: 0.002213  eta: 0h8m  tot: 0h7m10s  (45.4%)28.5%  lr: 0.026987  loss: 0.002214  eta: 0h8m  tot: 0h7m13s  (45.7%)%  lr: 0.026807  loss: 0.002212  eta: 0h8m  tot: 0h7m16s  (46.0%)0.002202  eta: 0h8m  tot: 0h7m24s  (47.0%)35.4%  lr: 0.026196  loss: 0.002191  eta: 0h8m  tot: 0h7m25s  (47.1%)36.3%  lr: 0.026156  loss: 0.002185  eta: 0h8m  tot: 0h7m27s  (47.3%)37.8%  lr: 0.026016  loss: 0.002180  eta: 0h8m  tot: 0h7m30s  (47.6%)38.2%  lr: 0.025936  loss: 0.002170  eta: 0h8m  tot: 0h7m30s  (47.6%)40.3%  lr: 0.025726  loss: 0.002159  eta: 0h7m  tot: 0h7m35s  (48.1%)41.6%  lr: 0.025546  loss: 0.002166  eta: 0h7m  tot: 0h7m37s  (48.3%)43.9%  lr: 0.025305  loss: 0.002151  eta: 0h7m  tot: 0h7m42s  (48.8%)44.9%  lr: 0.025165  loss: 0.002153  eta: 0h7m  tot: 0h7m44s  (49.0%)47.1%  lr: 0.024925  loss: 0.002155  eta: 0h7m  tot: 0h7m49s  (49.4%)%  lr: 0.024755  loss: 0.002144  eta: 0h7m  tot: 0h7m51s  (49.7%)49.0%  lr: 0.024685  loss: 0.002128  eta: 0h7m  tot: 0h7m53s  (49.8%)51.9%  lr: 0.024354  loss: 0.002115  eta: 0h7m  tot: 0h7m59s  (50.4%)52.8%  lr: 0.024234  loss: 0.002114  eta: 0h7m  tot: 0h8m0s  (50.6%)53.4%  lr: 0.024184  loss: 0.002105  eta: 0h7m  tot: 0h8m2s  (50.7%)54.8%  lr: 0.024074  loss: 0.002103  eta: 0h7m  tot: 0h8m4s  (51.0%)54.9%  lr: 0.024054  loss: 0.002103  eta: 0h7m  tot: 0h8m4s  (51.0%)55.5%  lr: 0.023974  loss: 0.002099  eta: 0h7m  tot: 0h8m5s  (51.1%)56.1%  lr: 0.023944  loss: 0.002103  eta: 0h7m  tot: 0h8m6s  (51.2%)62.0%  lr: 0.023564  loss: 0.002125  eta: 0h7m  tot: 0h8m17s  (52.4%)62.7%  lr: 0.023524  loss: 0.002129  eta: 0h7m  tot: 0h8m19s  (52.5%)64.6%  lr: 0.023353  loss: 0.002126  eta: 0h7m  tot: 0h8m22s  (52.9%)65.8%  lr: 0.023283  loss: 0.002116  eta: 0h7m  tot: 0h8m24s  (53.2%)66.5%  lr: 0.023233  loss: 0.002114  eta: 0h7m  tot: 0h8m25s  (53.3%)66.7%  lr: 0.023193  loss: 0.002114  eta: 0h7m  tot: 0h8m26s  (53.3%)68.1%  lr: 0.023113  loss: 0.002113  eta: 0h7m  tot: 0h8m28s  (53.6%)68.4%  lr: 0.023083  loss: 0.002119  eta: 0h7m  tot: 0h8m29s  (53.7%)8m29s  (53.7%)74.7%  lr: 0.022403  loss: 0.002115  eta: 0h7m  tot: 0h8m40s  (54.9%)0.002111  eta: 0h6m  tot: 0h8m44s  (55.4%)78.9%  lr: 0.022042  loss: 0.002113  eta: 0h6m  tot: 0h8m48s  (55.8%)79.8%  lr: 0.021912  loss: 0.002111  eta: 0h6m  tot: 0h8m49s  (56.0%)%  lr: 0.021762  loss: 0.002107  eta: 0h6m  tot: 0h8m53s  (56.3%)0h6m  tot: 0h8m53s  (56.4%)82.7%  lr: 0.021682  loss: 0.002109  eta: 0h6m  tot: 0h8m55s  (56.5%)82.9%  lr: 0.021632  loss: 0.002110  eta: 0h6m  tot: 0h8m55s  (56.6%)83.1%  lr: 0.021602  loss: 0.002113  eta: 0h6m  tot: 0h8m55s  (56.6%)85.0%  lr: 0.021382  loss: 0.002121  eta: 0h6m  tot: 0h8m59s  (57.0%)89.5%  lr: 0.021001  loss: 0.002120  eta: 0h6m  tot: 0h9m8s  (57.9%)94.3%  lr: 0.020521  loss: 0.002145  eta: 0h6m  tot: 0h9m17s  (58.9%)94.9%  lr: 0.020441  loss: 0.002143  eta: 0h6m  tot: 0h9m18s  (59.0%)95.1%  lr: 0.020441  loss: 0.002141  eta: 0h6m  tot: 0h9m19s  (59.0%)95.6%  lr: 0.020381  loss: 0.002141  eta: 0h6m  tot: 0h9m20s  (59.1%)95.8%  lr: 0.020361  loss: 0.002139  eta: 0h6m  tot: 0h9m20s  (59.2%)0.020350  loss: 0.002138  eta: 0h6m  tot: 0h9m21s  (59.2%)96.4%  lr: 0.020280  loss: 0.002135  eta: 0h6m  tot: 0h9m22s  (59.3%)%  lr: 0.020120  loss: 0.002142  eta: 0h6m  tot: 0h9m25s  (59.7%)  lr: 0.020030  loss: 0.002140  eta: 0h6m  tot: 0h9m27s  (59.8%)\n",
      " ---+++                Epoch    2 Train error : 0.00218800 +++--- ���\n",
      "Training epoch 3: 0.02 0.01\n",
      "Epoch: 100.0%  lr: 0.010000  loss: 0.001845  eta: 0h3m  tot: 0h12m34s  (80.0%)2%)3.2%  lr: 0.019680  loss: 0.001536  eta: 0h6m  tot: 0h9m35s  (60.6%)4.4%  lr: 0.019489  loss: 0.001556  eta: 0h6m  tot: 0h9m37s  (60.9%)5.3%  lr: 0.019359  loss: 0.001534  eta: 0h6m  tot: 0h9m38s  (61.1%)%  lr: 0.018759  loss: 0.001853  eta: 0h6m  tot: 0h9m49s  (62.2%)0.001889  eta: 0h5m  tot: 0h9m53s  (62.7%)  lr: 0.018549  loss: 0.001894  eta: 0h5m  tot: 0h9m53s  (62.7%)17.1%  lr: 0.018178  loss: 0.001888  eta: 0h5m  tot: 0h10m1s  (63.4%)18.1%  lr: 0.018128  loss: 0.001910  eta: 0h5m  tot: 0h10m2s  (63.6%)18.8%  lr: 0.018028  loss: 0.001932  eta: 0h5m  tot: 0h10m3s  (63.8%)%  lr: 0.017878  loss: 0.001888  eta: 0h5m  tot: 0h10m5s  (64.0%)20.5%  lr: 0.017818  loss: 0.001883  eta: 0h5m  tot: 0h10m6s  (64.1%)20.9%  lr: 0.017798  loss: 0.001915  eta: 0h5m  tot: 0h10m7s  (64.2%)22.6%  lr: 0.017688  loss: 0.001896  eta: 0h5m  tot: 0h10m10s  (64.5%)25.3%  lr: 0.017417  loss: 0.001888  eta: 0h5m  tot: 0h10m15s  (65.1%)26.0%  lr: 0.017267  loss: 0.001874  eta: 0h5m  tot: 0h10m16s  (65.2%)26.4%  lr: 0.017207  loss: 0.001861  eta: 0h5m  tot: 0h10m17s  (65.3%)27.0%  lr: 0.017127  loss: 0.001859  eta: 0h5m  tot: 0h10m18s  (65.4%)27.9%  lr: 0.017037  loss: 0.001849  eta: 0h5m  tot: 0h10m20s  (65.6%)29.5%  lr: 0.016907  loss: 0.001861  eta: 0h5m  tot: 0h10m22s  (65.9%)30.0%  lr: 0.016837  loss: 0.001861  eta: 0h5m  tot: 0h10m23s  (66.0%)30.4%  lr: 0.016827  loss: 0.001871  eta: 0h5m  tot: 0h10m24s  (66.1%)31.0%  lr: 0.016797  loss: 0.001881  eta: 0h5m  tot: 0h10m25s  (66.2%)%  lr: 0.016707  loss: 0.001892  eta: 0h5m  tot: 0h10m26s  (66.3%)0.001866  eta: 0h5m  tot: 0h10m29s  (66.7%)%  lr: 0.016577  loss: 0.001862  eta: 0h5m  tot: 0h10m30s  (66.7%)35.2%  lr: 0.016436  loss: 0.001864  eta: 0h5m  tot: 0h10m33s  (67.0%)36.5%  lr: 0.016346  loss: 0.001859  eta: 0h5m  tot: 0h10m35s  (67.3%)%  lr: 0.015986  loss: 0.001876  eta: 0h4m  tot: 0h10m42s  (68.0%)41.1%  lr: 0.015816  loss: 0.001871  eta: 0h4m  tot: 0h10m44s  (68.2%)41.8%  lr: 0.015726  loss: 0.001874  eta: 0h4m  tot: 0h10m46s  (68.4%)42.2%  lr: 0.015706  loss: 0.001883  eta: 0h4m  tot: 0h10m46s  (68.4%)43.7%  lr: 0.015546  loss: 0.001890  eta: 0h4m  tot: 0h10m49s  (68.7%)43.9%  lr: 0.015526  loss: 0.001892  eta: 0h4m  tot: 0h10m50s  (68.8%)%  lr: 0.015446  loss: 0.001897  eta: 0h4m  tot: 0h10m51s  (69.0%)45.2%  lr: 0.015415  loss: 0.001893  eta: 0h4m  tot: 0h10m52s  (69.0%)45.8%  lr: 0.015345  loss: 0.001887  eta: 0h4m  tot: 0h10m53s  (69.2%)48.5%  lr: 0.015065  loss: 0.001889  eta: 0h4m  tot: 0h10m58s  (69.7%)54.2%  lr: 0.014475  loss: 0.001872  eta: 0h4m  tot: 0h11m8s  (70.8%)56.1%  lr: 0.014284  loss: 0.001850  eta: 0h4m  tot: 0h11m12s  (71.2%)0.001844  eta: 0h4m  tot: 0h11m13s  (71.4%)57.4%  lr: 0.014134  loss: 0.001852  eta: 0h4m  tot: 0h11m14s  (71.5%)58.2%  lr: 0.014074  loss: 0.001853  eta: 0h4m  tot: 0h11m16s  (71.6%)%)66.2%  lr: 0.013273  loss: 0.001889  eta: 0h4m  tot: 0h11m31s  (73.2%)66.9%  lr: 0.013143  loss: 0.001887  eta: 0h4m  tot: 0h11m33s  (73.4%)%  lr: 0.013083  loss: 0.001884  eta: 0h4m  tot: 0h11m34s  (73.5%)69.4%  lr: 0.012853  loss: 0.001876  eta: 0h4m  tot: 0h11m38s  (73.9%)69.6%  lr: 0.012843  loss: 0.001878  eta: 0h4m  tot: 0h11m38s  (73.9%)69.8%  lr: 0.012803  loss: 0.001883  eta: 0h4m  tot: 0h11m38s  (74.0%)72.8%  lr: 0.012523  loss: 0.001891  eta: 0h3m  tot: 0h11m44s  (74.6%)%  lr: 0.011652  loss: 0.001872  eta: 0h3m  tot: 0h12m0s  (76.3%)0.011472  loss: 0.001863  eta: 0h3m  tot: 0h12m3s  (76.7%)%  lr: 0.011422  loss: 0.001867  eta: 0h3m  tot: 0h12m4s  (76.7%)87.6%  lr: 0.010981  loss: 0.001864  eta: 0h3m  tot: 0h12m12s  (77.5%)90.9%  lr: 0.010661  loss: 0.001860  eta: 0h3m  tot: 0h12m18s  (78.2%)%  lr: 0.010591  loss: 0.001859  eta: 0h3m  tot: 0h12m19s  (78.2%)0.001858  eta: 0h3m  tot: 0h12m19s  (78.3%)94.7%  lr: 0.010340  loss: 0.001856  eta: 0h3m  tot: 0h12m26s  (78.9%)79.2%)97.3%  lr: 0.010120  loss: 0.001849  eta: 0h3m  tot: 0h12m31s  (79.5%)97.7%  lr: 0.010110  loss: 0.001850  eta: 0h3m  tot: 0h12m32s  (79.5%)98.7%  lr: 0.010030  loss: 0.001845  eta: 0h3m  tot: 0h12m33s  (79.7%)98.9%  lr: 0.010020  loss: 0.001846  eta: 0h3m  tot: 0h12m33s  (79.8%)\n",
      " ---+++                Epoch    3 Train error : 0.00183795 +++--- ���\n",
      "Training epoch 4: 0.01 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100.0%  lr: 0.000000  loss: 0.001538  eta: <1min   tot: 0h15m38s  (100.0%) lr: 0.009970  loss: 0.001096  eta: 0h3m  tot: 0h12m34s  (80.1%)0.6%  lr: 0.009960  loss: 0.001894  eta: 0h3m  tot: 0h12m35s  (80.1%)0.9%  lr: 0.009920  loss: 0.001584  eta: 0h3m  tot: 0h12m36s  (80.2%)2.7%  lr: 0.009770  loss: 0.001391  eta: 0h3m  tot: 0h12m39s  (80.5%)%  lr: 0.009700  loss: 0.001366  eta: 0h3m  tot: 0h12m40s  (80.6%)3.6%  lr: 0.009680  loss: 0.001488  eta: 0h3m  tot: 0h12m41s  (80.7%)5.1%  lr: 0.009580  loss: 0.001478  eta: 0h3m  tot: 0h12m44s  (81.0%)  loss: 0.001451  eta: 0h3m  tot: 0h12m46s  (81.2%)6.1%  lr: 0.009500  loss: 0.001436  eta: 0h3m  tot: 0h12m46s  (81.2%)9.3%  lr: 0.009089  loss: 0.001389  eta: 0h2m  tot: 0h12m52s  (81.9%)%  lr: 0.008919  loss: 0.001457  eta: 0h2m  tot: 0h12m56s  (82.3%)12.0%  lr: 0.008839  loss: 0.001494  eta: 0h2m  tot: 0h12m57s  (82.4%)13.3%  lr: 0.008719  loss: 0.001556  eta: 0h2m  tot: 0h12m59s  (82.7%)13.9%  lr: 0.008649  loss: 0.001552  eta: 0h2m  tot: 0h13m0s  (82.8%)%  lr: 0.008549  loss: 0.001519  eta: 0h2m  tot: 0h13m2s  (83.0%)15.0%  lr: 0.008519  loss: 0.001529  eta: 0h2m  tot: 0h13m2s  (83.0%)15.2%  lr: 0.008509  loss: 0.001521  eta: 0h2m  tot: 0h13m2s  (83.0%)2m  tot: 0h13m6s  (83.4%)%  lr: 0.008208  loss: 0.001526  eta: 0h2m  tot: 0h13m8s  (83.6%)22.4%  lr: 0.007628  loss: 0.001497  eta: 0h2m  tot: 0h13m16s  (84.5%)23.2%  lr: 0.007538  loss: 0.001513  eta: 0h2m  tot: 0h13m18s  (84.6%)  loss: 0.001514  eta: 0h2m  tot: 0h13m19s  (84.7%)2m  tot: 0h13m20s  (84.9%)25.1%  lr: 0.007337  loss: 0.001512  eta: 0h2m  tot: 0h13m21s  (85.0%)%  lr: 0.007327  loss: 0.001511  eta: 0h2m  tot: 0h13m22s  (85.1%)26.8%  lr: 0.007207  loss: 0.001481  eta: 0h2m  tot: 0h13m25s  (85.4%)%  lr: 0.007177  loss: 0.001482  eta: 0h2m  tot: 0h13m25s  (85.4%)m  tot: 0h13m29s  (85.9%)30.0%  lr: 0.006827  loss: 0.001469  eta: 0h2m  tot: 0h13m30s  (86.0%)31.6%  lr: 0.006607  loss: 0.001457  eta: 0h2m  tot: 0h13m33s  (86.3%)32.7%  lr: 0.006467  loss: 0.001445  eta: 0h2m  tot: 0h13m35s  (86.5%)%  lr: 0.006436  loss: 0.001446  eta: 0h2m  tot: 0h13m36s  (86.6%)33.3%  lr: 0.006426  loss: 0.001443  eta: 0h2m  tot: 0h13m36s  (86.7%)33.8%  lr: 0.006396  loss: 0.001445  eta: 0h2m  tot: 0h13m37s  (86.8%)34.6%  lr: 0.006356  loss: 0.001446  eta: 0h2m  tot: 0h13m39s  (86.9%)35.2%  lr: 0.006336  loss: 0.001438  eta: 0h2m  tot: 0h13m40s  (87.0%)%  lr: 0.006286  loss: 0.001449  eta: 0h2m  tot: 0h13m41s  (87.1%)%  lr: 0.006206  loss: 0.001456  eta: 0h1m  tot: 0h13m43s  (87.4%)%  lr: 0.006006  loss: 0.001453  eta: 0h1m  tot: 0h13m46s  (87.8%)39.4%  lr: 0.005976  loss: 0.001455  eta: 0h1m  tot: 0h13m47s  (87.9%)39.5%  lr: 0.005966  loss: 0.001456  eta: 0h1m  tot: 0h13m47s  (87.9%)39.7%  lr: 0.005956  loss: 0.001453  eta: 0h1m  tot: 0h13m48s  (87.9%)41.4%  lr: 0.005856  loss: 0.001493  eta: 0h1m  tot: 0h13m51s  (88.3%)43.3%  lr: 0.005696  loss: 0.001495  eta: 0h1m  tot: 0h13m54s  (88.7%)45.2%  lr: 0.005486  loss: 0.001501  eta: 0h1m  tot: 0h13m58s  (89.0%)45.6%  lr: 0.005425  loss: 0.001502  eta: 0h1m  tot: 0h13m58s  (89.1%)46.6%  lr: 0.005385  loss: 0.001505  eta: 0h1m  tot: 0h14m0s  (89.3%)%  lr: 0.005355  loss: 0.001510  eta: 0h1m  tot: 0h14m1s  (89.4%)47.5%  lr: 0.005295  loss: 0.001507  eta: 0h1m  tot: 0h14m2s  (89.5%)0.001521  eta: 0h1m  tot: 0h14m10s  (90.3%)52.5%  lr: 0.004805  loss: 0.001515  eta: 0h1m  tot: 0h14m12s  (90.5%)53.0%  lr: 0.004705  loss: 0.001523  eta: 0h1m  tot: 0h14m14s  (90.6%)56.3%  lr: 0.004344  loss: 0.001545  eta: 0h1m  tot: 0h14m20s  (91.3%)91.6%)58.2%  lr: 0.004084  loss: 0.001574  eta: 0h1m  tot: 0h14m23s  (91.6%)59.3%  lr: 0.003994  loss: 0.001579  eta: 0h1m  tot: 0h14m25s  (91.9%)63.3%  lr: 0.003644  loss: 0.001572  eta: 0h1m  tot: 0h14m32s  (92.7%)63.7%  lr: 0.003594  loss: 0.001569  eta: 0h1m  tot: 0h14m33s  (92.7%)64.3%  lr: 0.003524  loss: 0.001568  eta: 0h1m  tot: 0h14m34s  (92.9%)64.8%  lr: 0.003474  loss: 0.001568  eta: 0h1m  tot: 0h14m35s  (93.0%)65.4%  lr: 0.003414  loss: 0.001570  eta: 0h1m  tot: 0h14m36s  (93.1%)65.8%  lr: 0.003373  loss: 0.001565  eta: 0h1m  tot: 0h14m37s  (93.2%)66.3%  lr: 0.003323  loss: 0.001565  eta: 0h1m  tot: 0h14m38s  (93.3%)69.8%  lr: 0.003073  loss: 0.001560  eta: <1min   tot: 0h14m44s  (94.0%)70.0%  lr: 0.003053  loss: 0.001558  eta: <1min   tot: 0h14m44s  (94.0%)70.3%  lr: 0.003053  loss: 0.001557  eta: <1min   tot: 0h14m45s  (94.1%)70.5%  lr: 0.003033  loss: 0.001555  eta: <1min   tot: 0h14m45s  (94.1%)75.7%  lr: 0.002433  loss: 0.001572  eta: <1min   tot: 0h14m55s  (95.1%)0.001563  eta: <1min   tot: 0h15m4s  (96.1%)82.7%  lr: 0.001652  loss: 0.001559  eta: <1min   tot: 0h15m8s  (96.5%)83.1%  lr: 0.001582  loss: 0.001560  eta: <1min   tot: 0h15m9s  (96.6%)83.6%  lr: 0.001542  loss: 0.001557  eta: <1min   tot: 0h15m10s  (96.7%)86.1%  lr: 0.001291  loss: 0.001559  eta: <1min   tot: 0h15m14s  (97.2%)86.7%  lr: 0.001251  loss: 0.001559  eta: <1min   tot: 0h15m15s  (97.3%)89.5%  lr: 0.000931  loss: 0.001555  eta: <1min   tot: 0h15m20s  (97.9%)%  lr: 0.000841  loss: 0.001551  eta: <1min   tot: 0h15m22s  (98.1%)90.7%  lr: 0.000831  loss: 0.001553  eta: <1min   tot: 0h15m22s  (98.1%)90.9%  lr: 0.000821  loss: 0.001553  eta: <1min   tot: 0h15m23s  (98.2%)93.7%  lr: 0.000521  loss: 0.001544  eta: <1min   tot: 0h15m28s  (98.7%)94.3%  lr: 0.000451  loss: 0.001543  eta: <1min   tot: 0h15m29s  (98.9%)95.8%  lr: 0.000320  loss: 0.001540  eta: <1min   tot: 0h15m32s  (99.2%)96.0%  lr: 0.000300  loss: 0.001540  eta: <1min   tot: 0h15m32s  (99.2%)\n",
      " ---+++                Epoch    4 Train error : 0.00164648 +++--- ���\n",
      "Saving model to file : data/starspace_embeddings\n",
      "Saving model in tsv format : data/starspace_embeddings.tsv\n"
     ]
    }
   ],
   "source": [
    "!starspace train \\\n",
    "  -trainFile data/prepared_train.tsv \\\n",
    "  -model data/starspace_embeddings \\\n",
    "  -trainMode 3 \\\n",
    "  -initRandSd 0.01 \\\n",
    "  -adagrad true \\\n",
    "  -ngrams 1 \\\n",
    "  -lr 0.05 \\\n",
    "  -epoch 5 \\\n",
    "  -thread 20 \\\n",
    "  -dim 100 \\\n",
    "  -negSearchLimit 10 \\\n",
    "  -fileFormat labelDoc \\\n",
    "  -similarity \"cosine\" \\\n",
    "  -minCount 5 \\\n",
    "  -normalizeText true \\\n",
    "  -verbose true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "And now we can compare the new embeddings with the previous ones. You can find trained word vectors in the file *[model_file_name].tsv*. Upload the embeddings from StarSpace into a dict. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "starspace_embeddings = {}\n",
    "with open(\"data/starspace_embeddings.tsv\") as f:\n",
    "    for line in f:\n",
    "        line = line.split(\"\\t\")\n",
    "        starspace_embeddings[line[0]] = np.array([np.float(el) for el in line[1:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_prepared_ranking = []\n",
    "for line in prepared_validation:\n",
    "    q, *ex = line\n",
    "    ranks = rank_candidates(q, ex, starspace_embeddings, 100)\n",
    "    ss_prepared_ranking.append([r[0] for r in ranks].index(0) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCG@   1: 0.516 | Hits@   1: 0.516\n",
      "DCG@   5: 0.613 | Hits@   5: 0.695\n",
      "DCG@  10: 0.631 | Hits@  10: 0.751\n",
      "DCG@ 100: 0.661 | Hits@ 100: 0.898\n",
      "DCG@ 500: 0.672 | Hits@ 500: 0.979\n",
      "DCG@1000: 0.674 | Hits@1000: 1.000\n"
     ]
    }
   ],
   "source": [
    "for k in [1, 5, 10, 100, 500, 1000]:\n",
    "    print(\"DCG@%4d: %.3f | Hits@%4d: %.3f\" % (k, dcg_score(ss_prepared_ranking, k), \n",
    "                                               k, hits_count(ss_prepared_ranking, k)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to training for the particular task with the supervised data, you should expect to obtain a higher quality than for the previous approach. In additiion, despite the fact that StarSpace's trained vectors have a smaller dimension than word2vec's, it provides better results in this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 5 (StarSpaceRanks).** For each question from prepared *test.tsv* submit the ranks of the candidates for trained representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current answer for task StarSpaceRanks is: 83\t54\t67\t33\t14\t52\t68\t88\t66\t85\t1\t41\t6\t81\t93\t38\t7\t47\t24\t75\t62\t56\t12\t90\t84\t9\t91\t100\t20\t4\t27\t78\t25\t70\t80...\n"
     ]
    }
   ],
   "source": [
    "starspace_ranks_results = []\n",
    "prepared_test_data = \"data/prepared_test.tsv\"\n",
    "for line in open(prepared_test_data):\n",
    "    q, *ex = line.strip().split('\\t')\n",
    "    ranks = rank_candidates(q, ex, starspace_embeddings, 100)\n",
    "    ranked_candidates = [r[0] for r in ranks]\n",
    "    starspace_ranks_results.append([ranked_candidates.index(i) + 1 for i in range(len(ranked_candidates))])\n",
    "    \n",
    "grader.submit_tag('StarSpaceRanks', matrix_to_string(starspace_ranks_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please, **don't remove** the file with these embeddings because you will need them in the final project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authorization & Submission\n",
    "To submit assignment parts to Cousera platform, please, enter your e-mail and token into variables below. You can generate token on this programming assignment page. <b>Note:</b> Token expires 30 minutes after generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You want to submit these parts:\n",
      "Task Question2Vec: 0.019293891059\n",
      "-0.0287272135417\n",
      "0.0460561116536\n",
      "0.0852593315972\n",
      "0.0243055555556\n",
      "-0.0729031032986\n",
      "0.0...\n",
      "Task HitsCount: 1.0\n",
      "0.5\n",
      "1.0\n",
      "0.5\n",
      "1.0\n",
      "0.3333333333333333\n",
      "0.6666666666666666\n",
      "1.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "1....\n",
      "Task DCGScore: 1.0\n",
      "0.5\n",
      "0.815464876786\n",
      "0.5\n",
      "0.815464876786\n",
      "0.333333333333\n",
      "0.54364325119\n",
      "0.710309917857\n",
      "0.1\n",
      "0.16309297...\n",
      "Task W2VTokenizedRanks: 95\t94\t7\t9\t64\t36\t31\t93\t23\t100\t99\t20\t60\t6\t97\t48\t70\t37\t41\t96\t29\t56\t2\t65\t68\t44\t27\t25\t57\t62\t11\t87\t50\t66\t7...\n",
      "Task StarSpaceRanks: 83\t54\t67\t33\t14\t52\t68\t88\t66\t85\t1\t41\t6\t81\t93\t38\t7\t47\t24\t75\t62\t56\t12\t90\t84\t9\t91\t100\t20\t4\t27\t78\t25\t70\t80...\n"
     ]
    }
   ],
   "source": [
    "STUDENT_EMAIL = \"klulu@kse.org.ua\"\n",
    "STUDENT_TOKEN = \"uq33LwFgs9HG0Jrs\"\n",
    "grader.status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to submit these answers, run cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted to Coursera platform. See results on assignment page!\n"
     ]
    }
   ],
   "source": [
    "grader.submit(STUDENT_EMAIL, STUDENT_TOKEN)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
